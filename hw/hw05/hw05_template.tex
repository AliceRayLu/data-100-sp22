% DS100: Search for YOUR ANSWER HERE in this LaTeX document. Then (if using Overleaf) press "Recompile" or Ctrl/Cmd+S.
% To download a file, select the download button next to "Recompile"
% http://mirrors.concertpass.com/tex-archive/macros/latex/contrib/exam/examdoc.pdf

\documentclass[addpoints, 12pt]{exam}

\usepackage{multicol}
\usepackage{enumitem}
\usepackage{bbm}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{url}
\usepackage{multicol}
\input{macros}

\lstset{language=python, basicstyle=\ttfamily}
\newcommand{\closedinterval}[2]{\left[#1, #2\right]}
\setlength{\columnsep}{1cm}
\setlength{\parskip}{1em}
\usepackage{framed}

\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem*{answer}{Answer}


\begin{document}


\definecolor{shadecolor}{gray}{0.95}
\newcommand{\homework}{5}
\newcommand{\duedate}{Thursday, March 3rd at 11:59 PM Pacific}
\lecture{Homework \#\homework{}}{}{Due Date: \duedate{}}{}

\noindent\textbf{Total Points: 36}

\fullwidth{\section*{Submission Instructions}}

\noindent You must submit this assignment to Gradescope by \textbf{\duedate{}}. While Gradescope accepts late submissions, you will not receive \textbf{any} credit for a late submission if you do not have prior accommodations (e.g. DSP).


\vspace*{1em}
\noindent You can work on this assignment in any way you like:

\begin{itemize}
    \item One way is to download this PDF, print it out, and write directly on these pages (we've provided enough space for you to do so). Alternatively, if you have a tablet, you could save this PDF and write directly on it.
    \item Another way is to use some form of LaTeX. Overleaf is a great tool; visit the course website for a LaTeX template of this homework.
    \item You could also write your answers on a blank sheet of paper.
\end{itemize}

\noindent Regardless of what method you choose, the end result needs to end up on Gradescope, as a PDF. If you wrote something on physical paper (like options 1 and 3 above), you will need to use a scanning application (e.g. CamScanner) in order to submit your work. 

\vspace*{1em}
\noindent When submitting on Gradescope, you \textbf{must correctly assign pages to each question} (it prompts you to do this after submitting your work). This significantly streamlines the grading process for our tutors. Failure to do this may result in a score of 0 for any questions that you didn't correctly assign pages to. If you have any questions about the submission process, please don't hesitate to ask on Piazza.

\fullwidth{\section*{Collaborators}}
\noindent Data science is a collaborative activity. While you may talk with others about the homework, we ask that you write your solutions individually. If you do discuss the assignments with others please include their names at the top of your submission.

\newpage

\begin{questions}
\fullwidth{\section*{Properties of Simple Linear Regression}}

\vspace{-1em}

\question[7] In lecture, we spent a great deal of time talking about  simple linear regression, which you also saw in Data 8. To briefly summarize, the simple linear regression model assumes that given a single observation $x$, our predicted response for this observation is $\hat{y} = \theta_0 + \theta_1x$. {\small (Note: In this problem we write $(\theta_0, \theta_1)$ instead of $(a, b)$ to more closely mirror the multiple linear regression model notation.)}

In Lecture 9 we saw that the $\theta_0 = \hat{\theta_0}$ and $\theta_1 = \hat{\theta_1}$ that minimize the average $L_2$ loss for the simple linear regression model are:

$$\hat{\theta_0} = \bar{y} - \hat{\theta_1}\bar{x} $$
$$\hat{\theta_1} = r\frac{\sigma_y}{\sigma_x}$$

Or, rearranging terms, our predictions $\hat{y}$ are:

$$\hat{y} = \bar{y} + r \sigma_y \frac{x - \bar{x}}{\sigma_x}$$

\begin{parts}

%%%%%%%%%%%%%%%%%% 1(a)
\part[3] As we saw in lecture, a residual $e_i$ is defined to be the difference between a true response $y_i$ and predicted response $\hat{y_i}$. Specifically, $e_i = y_i - \hat{y_i}$. Note that there are $n$ data points, and each data point is denoted by $(x_i, y_i)$.

Prove, using the equation for $\hat{y}$ above, that $\sum_{i = 1}^n e_i$ = 0 (meaning the sum of the residuals is zero).
    \begin{shaded}
    \begin{answer}

% YOUR ANSWER HERE %

    \end{answer}
    \end{shaded}


%%%%%%%%%%%%%%%%%% 1(b)
\part[2] Using your result from part (a), prove that $\bar{y} = \bar{\hat{y}}$.
    \begin{shaded}
    \begin{answer}

% YOUR ANSWER HERE %

    \end{answer}
    \end{shaded}


%%%%%%%%%%%%%%%%%% 1(c)
\newpage
\part[2] Prove that $(\bar{x}, \bar{y})$ is on the simple linear regression line.
    \begin{shaded}
    \begin{answer}

% YOUR ANSWER HERE %

    \end{answer}
    \end{shaded}

\end{parts}


\newpage

\fullwidth{\section*{Geometric Perspective of Least Squares}}


\question[7] In Lecture 11, we viewed both the simple linear regression model and the multiple linear regression model through the lens of linear algebra. The key geometric insight was that if we train a model on some design matrix $\Bbb{X}$ and true response vector $\Bbb{Y}$, our predicted response $\hat{\Bbb{Y}} = \Bbb{X} \hat{\theta}$ is the vector in $\text{span}(\Bbb{X})$ that is closest to $\Bbb{Y}$ ($\hat{\Bbb{Y}}$ is the orthogonal projection of $\Bbb{Y}$ onto the $\text{span}(\Bbb{X})$). 

In the simple linear regression case, our optimal vector $\theta$ is $\hat{\theta} = [\hat{\theta_0}, \hat{\theta_1}]^T$, and our design matrix is

$$\Bbb{X} = \begin{bmatrix} 1 & x_1 \\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_n \end{bmatrix} = \begin{bmatrix} | & | \\ \mathbbm{1} & \vec{x} \\ | & | \end{bmatrix}$$

This means we can write our predicted response vector as $\hat{\Bbb{Y}} = \Bbb{X} \begin{bmatrix} \hat{\theta_0} \\ \hat{\theta_1} \end{bmatrix} = \hat{\theta_0} \mathbbm{1} + \hat{\theta_1} \vec{x}$. 

Note, in this problem, $\vec{x}$ refers to the $n$-length vector $[x_1, x_2, ..., x_n]^T$. In other words, it is a feature, not an observation.

For this problem, assume we are working with the \textbf{simple linear regression model}, though the properties we establish here hold for any linear regression model that contains an intercept term.

\begin{parts}
%%%%%%%%%%%%%%%%%% 2(a)
\part[3] Using the geometric properties from lecture, prove that $\sum\limits_{i = 1}^n e_i = 0$. \\
\textit{Hint:} Recall, we define the residual vector as $e = \Bbb{Y} - \Bbb{\hat{Y}}$, and $e = [e_1, e_2, ..., e_n]^T$.
    \begin{shaded}
    \begin{answer}

% YOUR ANSWER HERE %

    \end{answer}
    \end{shaded}

%%%%%%%%%%%%%%%%%% 2(b)
\part[2] Explain why the vector $\vec{x}$ (as defined in the problem) and the residual vector $e$ are orthogonal. \textit{Hint: Two vectors are orthogonal if their dot product is 0.}
    \begin{shaded}
    \begin{answer}

% YOUR ANSWER HERE %

    \end{answer}
    \end{shaded}


%%%%%%%%%%%%%%%%%% 2(c) 
\part[2] Explain why the predicted response vector $\hat{\Bbb{Y}}$ and the residual vector $e$ are orthogonal.
    \begin{shaded}
    \begin{answer}

% YOUR ANSWER HERE %

    \end{answer}
    \end{shaded}


\end{parts}
\end{questions}

\newpage

\fullwidth{\section*{Properties of a Linear Model With No Constant Term}}
\noindent Suppose that we don't include an intercept term in
our model. That is, our model is now
$$\hat{y} = \gamma x,$$
where $\gamma$ is the single parameter for our model that we need to optimize. (In this equation, $x$ is a scalar, corresponding to a single observation.)

\noindent As usual, we are looking to find the value $\hat{\gamma}$ that minimizes the average $L_2$ loss (mean squared error) across our observed data $\{(x_i, y_i)\}, i = 1, \ldots, n$:

$$R(\gamma) = \frac{1}{n}\sum_{i=1}^n (y_i - \gamma x_i)^2$$

\noindent The normal equations derived in lecture no longer hold. In this problem, we'll derive a solution to this simpler model. We'll see that the least squares estimate of the slope in this model differs from the simple linear regression model, and will also explore whether or not our properties from the previous problem still hold.

\begin{questions}
%%%%%%%%%%%%%%%%%% 3
    \setcounter{question}{2} % question number offset
\question[4] Use calculus to find the minimizing $\hat{\gamma}$.
That is, prove that 
$$ \hat{\gamma} = \frac{\sum x_iy_i}{\sum x_i^2}$$

Note: This is the slope of our regression line, analogous to $\hat{\theta_1}$ from our simple linear regression model.
    \begin{shaded}
    \begin{answer}

% YOUR ANSWER HERE %

    \end{answer}
    \end{shaded}


\newpage
\question[8] 

For our new simplified model, our design matrix $\Bbb{X}$ is:

$$\Bbb{X} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} = \begin{bmatrix} | \\ \vec{x} \\ | \end{bmatrix}.$$

Therefore our predicted response vector $\hat{\Bbb{Y}}$ can be expressed as $\hat{\Bbb{Y}} = \hat{\gamma} \vec{x}$. ($\vec{x}$ here is defined the same way it was in Question 2.) 

Earlier in this homework, we established several properties that held true for the simple linear regression model that contained an intercept term. For each of the following four properties, state whether or not they still hold true even when there isn't an intercept term. Be sure to justify your answer.

\begin{parts}
%%%%%%%%%%%%%%%%%% 4(a)
\part[2] $\sum\limits_{i = 1}^n e_i = 0$.
    \begin{shaded}
    \begin{answer}

% YOUR ANSWER HERE %

    \end{answer}
    \end{shaded}

%%%%%%%%%%%%%%%%%% 4(b)
\part[2] The column vector $\vec{x}$ and the residual vector $e$ are orthogonal.

    \begin{shaded}
    \begin{answer}

% YOUR ANSWER HERE %

    \end{answer}
    \end{shaded}
%%%%%%%%%%%%%%%%%% 4(c)
\part[2] The predicted response vector $\hat{\Bbb{Y}}$ and the residual vector $e$ are orthogonal.
    \begin{shaded}
    \begin{answer}

% YOUR ANSWER HERE %

    \end{answer}
    \end{shaded}
%%%%%%%%%%%%%%%%%% 4(d)
\part[2] $(\bar{x}, \bar{y})$ is on the regression line.
    \begin{shaded}
    \begin{answer}

% YOUR ANSWER HERE %

    \end{answer}
    \end{shaded}

\end{parts}

\newpage

\fullwidth{\section*{MSE ``Minimizer''}}
\question[10]{Recall from calculus that given some function $g(x)$, the $x$ you get from solving $\frac{d g(x)}{dx} = 0$ is called a \textit{critical point}
of $g$ -- this means it could be a minimizer or a maximizer for $g$. In this question, we will explore some basic properties and 
build some intuition on why, for certain loss functions such as squared $L_2$ loss, the critical point of the empirical risk function (defined as average loss on the observed data) will always be the minimizer.

Given some linear model $f(x) = \gamma x$ for some real scalar $\gamma$, we can write the empirical risk of the model $f$ given the observed data $\{x_i, y_i\}, i=1, \dots, n$ 
as the average $L_2$ loss, also known as mean squared error (MSE):
\begin{equation*}
    \frac{1}{n}\sum_{i=1}^n (y_i - \gamma x_i)^2.
\end{equation*}
}

\begin{parts}
%%%%%%%%%%%%%%%%%% 5(a)
\part[1] Let's break the function above into individual terms. Complete the following sentence by filling in the blanks using one of the options in the parenthesis following each of the blanks:
\vspace*{1em}

The mean squared error can be viewed as a sum of $n$ \_\_\_\_\_ (linear/quadratic/logarithmic/exponential) terms, each of which can be treated as a function of \_\_\_\_ ($x_i$/$y_i/\gamma$). 

    \begin{shaded}
    \begin{answer}

% YOUR ANSWER HERE %

    \end{answer}
    \end{shaded}
%%%%%%%%%%%%%%%%%% 5(b)
\part[3] Let's investigate one of the $n$ functions in the summation in the MSE. 
Define $g_i(\gamma) = \frac{1}{n}(y_i - \gamma x_i)^2$ for $i=1, \dots, n$. Recall from calculus that we can use
the 2nd derivative of a function to describe its curvature about a certain point (if it is facing concave up, down, or possibly a point of inflection). 
You can take the following as a fact: A function is convex if and only if the function's 2nd derivative is non-negative on its domain.
Based on this property, verify that $g_i$ is a \textbf{convex function}.

    \begin{shaded}
    \begin{answer}

% YOUR ANSWER HERE %

    \end{answer}
    \end{shaded}
%%%%%%%%%%%%%%%%%% 5(c)
\part[2] Briefly explain in words why given a
convex function $g(x)$, the critical point we get by solving $\frac{dg(x)}{dx} = 0$ minimizes $g$. You can assume that $\frac{dg(x)}{dx}$ is a function of $x$ (and not a constant).
    \begin{shaded}
    \begin{answer}

% YOUR ANSWER HERE %

    \end{answer}
    \end{shaded}
%%%%%%%%%%%%%%%%%% 5(d)
\part[3] Now that we have shown that each term in the summation of the MSE is a convex function, one might wonder if the entire summation is convex given that it is
a sum of convex functions. 
\vspace*{1em}

Let's look at the formal definition of a \textbf{convex function}.  Algebraically speaking, a function $g(x)$ is convex if for any two points $(x_1, g(x_1))$ and $(x_2, g(x_2))$ on the function,
    \begin{align*}
        g(cx_1 + (1-c)x_2) \le cg(x_1) + (1-c)g(x_2)
    \end{align*}
for any real constant $0 \le c \le 1$.
\vspace*{1em}
    
The above definition says that, given the plot
of a convex function $g(x)$, if you connect 2 randomly chosen points on the function, the line segment will always lie on or above $g(x)$ 
(try this with the graph of $y=x^2$).
\vspace*{1em}

\begin{subparts}
    \subpart[2] Using the definition above, show that if $g(x)$ and $h(x)$ are both convex functions, their sum $g(x) + h(x)$ will also be a convex function.

    \begin{shaded}
    \begin{answer}

    % YOUR ANSWER HERE %

    \end{answer}
    \end{shaded}


    \subpart[1] Based on what you have shown in the previous part, explain intuitively why the sum of $n$ convex functions is still a convex function when $n > 2$.
        \begin{shaded}
    \begin{answer}

% YOUR ANSWER HERE %

    \end{answer}
    \end{shaded}

\end{subparts}
%%%%%%%%%%%%%%%%%% 5(e)
\part[1] Finally, using the previous parts, explain why in our case that, when we solve for the critical point of the MSE by taking the gradient with respect to the parameter and setting the expression to $0$, it is guranteed that the solution we find will minimize the MSE.
    \begin{shaded}
    \begin{answer}

% YOUR ANSWER HERE %

    \end{answer}
    \end{shaded}

\end{parts}

\vspace*{1em}
\small{
Closing note: In this question, we have discussed only the simple linear model with no constant term---a single-variable function. However, the above properties extend more generally to all multivariable linear regression models; this proof is beyond the scope of this course and is left to a future you.
}
\end{questions}

\vfill
\begin{center}
    \textbf{
    Congratulations! You have finished Homework \homework{}!
    }
\end{center}
\end{document}